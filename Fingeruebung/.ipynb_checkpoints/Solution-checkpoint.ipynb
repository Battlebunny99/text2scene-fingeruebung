{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "surface-electric",
   "metadata": {},
   "source": [
    "# Text2Scene\n",
    "Dies ist die Lösung der Fingerübung für das Praktikum Text2Scene des Sommersemesters 2021\n",
    "\n",
    "Autor: Xuan Anh Nguyen <br>\n",
    "Email: xuananh6077@stud.uni-frankfurt.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mental-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.training import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "existing-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(ls):\n",
    "     x_set = set(ls)\n",
    "     x_dict = {}\n",
    "\n",
    "     for entry in x_set:\n",
    "          x_dict[entry] = ls.count(entry)\n",
    "\n",
    "     return x_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-correspondence",
   "metadata": {},
   "source": [
    "## Aufgabe 2.2 Vorverarbeitung\n",
    "Einlesen der Trainingsdaten sowie Training des Models mit dem NLP Paket **SpaCy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read all valid files used for training\n",
    "\n",
    "train_data = [] # save only valid data for training (i.e. important tags)\n",
    "full_data = [] # save all (xml) data\n",
    "poss_tags = []\n",
    "\n",
    "root = pathlib.Path().absolute()\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    # skip all hidden directories and files\n",
    "    files = [f for f in files if not f[0] == '.']\n",
    "    dirs[:] = [d for d in dirs if not d[0] == '.']\n",
    "    if not subdir.startswith('.'):\n",
    "        for filename in files:\n",
    "            filepath = subdir + os.sep + filename\n",
    "            if filepath.endswith(\".xml\"):\n",
    "                # filepath will now point towards a valid .xml file\n",
    "                \n",
    "                # read and parse xml files\n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                full_data.append(root)\n",
    "                \n",
    "                # label text with entities \n",
    "                entities = []\n",
    "                for elem in root[1]:\n",
    "                    # filter non usable entries\n",
    "                    if (elem.get('start') != None) and (elem.get('end') != None) and (elem.get('start') != '-1') and (elem.get('end') != '-1'):\n",
    "                        new_ent = (int(elem.get('start')), int(elem.get('end')), elem.tag)\n",
    "                        entities.append(new_ent)\n",
    "                    if elem.tag not in poss_tags:\n",
    "                        poss_tags.append(elem.tag)\n",
    "                        \n",
    "                # save the 2 special xml files extra so we can access them easier later\n",
    "                if filepath.endswith(\"Bicycles.xml\"):\n",
    "                    print(\"confirmed\")\n",
    "                    special1 = root\n",
    "                elif filepath.endswith(\"Highlights_of_the_Prado_Museum.xml\"):\n",
    "                    print(\"confirmed\")\n",
    "                    special2 = root\n",
    "\n",
    "                        \n",
    "                train_data.append((root[0].text, {'entities': entities}))\n",
    "    \n",
    "# print(TRAIN_DATA[1][0])\n",
    "print(poss_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model with our training data\n",
    "\"\"\"\n",
    "model = None\n",
    "n_iter = 1000\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model)  \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  \n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "#set up the pipeline\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "     \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        doc = nlp.make_doc(root[0].text)\n",
    "        for text, annotations in train_data:\n",
    "            try:\n",
    "                nlp.update(\n",
    "                    [Example.from_dict(nlp.make_doc(text), annotations)],  \n",
    "                    drop=0.4,  \n",
    "                    sgd=optimizer,\n",
    "                    losses=losses)\n",
    "            except:\n",
    "                pass\n",
    "        print('\\rEpisode {}\\tLosses: {:.2f}'.format(itn, losses['ner']), end=\"\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-evening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively load pretrained English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-earthquake",
   "metadata": {},
   "source": [
    "## 2.3 Auswertung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-norwegian",
   "metadata": {},
   "source": [
    "Wie oft kommen welche PoS-Tags vor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-arbitration",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = []\n",
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        pos.append(token.pos_)\n",
    "pos = count_pos(pos)\n",
    "print(\"Aufgabe 1:\")\n",
    "print(*pos.items(), sep=\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-ecuador",
   "metadata": {},
   "source": [
    "Wie viele [SpatialEntities, Places, Motions, Locations, Signals, QsLinks, OLinks] gibt es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungry-external",
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = []\n",
    "for data in full_data:\n",
    "    for elem in data[1]:\n",
    "         ents.append(elem.tag)\n",
    "\n",
    "ents = count_pos(ents)\n",
    "print(\"Aufgabe 2:\")\n",
    "print(*ents.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-buying",
   "metadata": {},
   "source": [
    "Wie oft kommen welche QsLink Typen vor? (DC,EC, ...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-numbers",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for data in full_data:\n",
    "    for elem in data[1]:\n",
    "        if elem.tag == \"QSLINK\":\n",
    "            links.append(elem.get('relType'))\n",
    "            \n",
    "links = count_pos(links)\n",
    "print(\"Aufgabe 3:\")\n",
    "print(*links.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-haiti",
   "metadata": {},
   "source": [
    "Verteilung der Satzlänge graphisch darstellen (x: Satzlänge, y: Wie häufig)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        # find length by counting whitespaces in the sentence\n",
    "        temp1 = sent.text\n",
    "        temp2 = temp1.replace(' ', '')\n",
    "        spaces = len(temp1) - len(temp2)\n",
    "        sents.append(spaces+1)\n",
    "# map lengths to amount of times it appereard\n",
    "distribution = count_pos(sents)\n",
    "plt.bar(list(distribution.keys()), list(distribution.values()))\n",
    "plt.title(\"Satzlängen\")\n",
    "plt.xlabel(\"Satzlängen\")\n",
    "plt.ylabel(\"Häufigkeiten\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-uniform",
   "metadata": {},
   "source": [
    "Welche Links (QSLinks, OLinks) werden von welchen Präpositionen (markiert durch SPATIAL_SIGNAL) getriggert (z.B. wie oft werden QSLinks durch die Präposition „on“ getriggert)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs_trigger = []\n",
    "o_trigger = []\n",
    "\n",
    "# collect all triggers\n",
    "for data in full_data:\n",
    "    temp_qs = []\n",
    "    temp_o = []\n",
    "    trigg_dic = {}\n",
    "    \n",
    "    # add all triggers ONLY with their id\n",
    "    for elem in data[1]:\n",
    "        if elem.tag == \"QSLINK\":\n",
    "            temp_qs.append(elem.attrib['trigger'])\n",
    "        elif elem.tag == \"OLINK\":\n",
    "            temp_o.append(elem.attrib['trigger'])\n",
    "        if elem.tag == \"SPATIAL_SIGNAL\":\n",
    "            trigg_dic[elem.get('id')] = elem.get('text')\n",
    "\n",
    "    # replace trigger id with their respective texts\n",
    "    for i in range(len(temp_qs)):\n",
    "        try:\n",
    "            qs_trigger.append(trigg_dic[temp_qs[i]])\n",
    "        except:\n",
    "            pass\n",
    "    for i in range(len(temp_o)):\n",
    "        try:\n",
    "            o_trigger.append(trigg_dic[temp_o[i]])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "qs_trigger = count_pos(qs_trigger)\n",
    "o_trigger = count_pos(o_trigger)\n",
    "\n",
    "print(\"Aufgabe 5:\")\n",
    "print(\"\\nQslink trigger:\", *qs_trigger.items(), sep=\"\\n\")\n",
    "print(\"\\nOlink trigger:\", *o_trigger.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-eugene",
   "metadata": {},
   "source": [
    "Welches sind die fünf häufigsten „MOTION“ Verben (und wie oft kommen diese vor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-sarah",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_verbs = []   # only collect lemma from verbs\n",
    "for data in full_data:\n",
    "    # collect all different motion verbs\n",
    "    verbs = []  \n",
    "    for elem in data[1]:\n",
    "        if elem.tag == \"MOTION\":\n",
    "            verbs.append(elem.attrib['text'])\n",
    "            \n",
    "    # only add their lemma to the actual list\n",
    "    doc = nlp(data[0].text)\n",
    "    for token in doc:\n",
    "        if token.text in verbs:\n",
    "            lemma_verbs.append(token.lemma_)\n",
    "\n",
    "lemma_verbs = count_pos(lemma_verbs)\n",
    "lemma_verbs = sorted(lemma_verbs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Aufgabe 6:\", *lemma_verbs[0:5], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scientific-subdivision",
   "metadata": {},
   "source": [
    "## 2.4 Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-darkness",
   "metadata": {},
   "source": [
    "Graphische Darstellung von Verbindungen zwischen Entitäten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-swedish",
   "metadata": {},
   "source": [
    "**Bicycle.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(special1)\n",
    "\n",
    "G = nx.Graph()\n",
    "color_map = []\n",
    "counter = 0\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"SPATIAL_ENTITY\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='r', with_labels=True)\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"PLACE\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='b', with_labels=True)\n",
    "\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"LOCATION\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='g', with_labels=True)\n",
    "\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"PATH\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='w', with_labels=True)\n",
    "\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"NONMOTIONEVENT\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='y', with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-murray",
   "metadata": {},
   "source": [
    "**Highlights_of_the_Prado_Museum.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southern-ozone",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "removable-scanner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-attention",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
