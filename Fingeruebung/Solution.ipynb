{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tired-retailer",
   "metadata": {},
   "source": [
    "# Text2Scene\n",
    "Dies ist die Lösung der Fingerübung für das Praktikum Text2Scene des Sommersemesters 2021\n",
    "\n",
    "Autor: Xuan Anh Nguyen <br>\n",
    "Email: xuananh6077@stud.uni-frankfurt.de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "enclosed-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import spacy\n",
    "import networkx as nx\n",
    "import xml.etree.ElementTree as ET\n",
    "import itertools\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from spacy.training import Example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "complex-budapest",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos(ls):\n",
    "     x_set = set(ls)\n",
    "     x_dict = {}\n",
    "\n",
    "     for entry in x_set:\n",
    "          x_dict[entry] = ls.count(entry)\n",
    "\n",
    "     return x_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-occasions",
   "metadata": {},
   "source": [
    "## Aufgabe 2.2 Vorverarbeitung\n",
    "Einlesen der Trainingsdaten sowie Training des Models mit dem NLP Paket **SpaCy**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "limiting-report",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confirmed\n",
      "confirmed\n",
      "['PLACE', 'PATH', 'SPATIAL_ENTITY', 'NONMOTION_EVENT', 'MOTION', 'SPATIAL_SIGNAL', 'MOTION_SIGNAL', 'MEASURE', 'QSLINK', 'OLINK', 'MOVELINK', 'MLINK', 'METALINK', 'MEASURELINK', 'CP', 'URL']\n"
     ]
    }
   ],
   "source": [
    "# read all valid files used for training\n",
    "\n",
    "train_data = [] # save only valid data for training (i.e. important tags)\n",
    "full_data = [] # save all (xml) data\n",
    "poss_tags = []\n",
    "\n",
    "root = pathlib.Path().absolute()\n",
    "for subdir, dirs, files in os.walk(root):\n",
    "    # skip all hidden directories and files\n",
    "    files = [f for f in files if not f[0] == '.']\n",
    "    dirs[:] = [d for d in dirs if not d[0] == '.']\n",
    "    if not subdir.startswith('.'):\n",
    "        for filename in files:\n",
    "            filepath = subdir + os.sep + filename\n",
    "            if filepath.endswith(\".xml\"):\n",
    "                # filepath will now point towards a valid .xml file\n",
    "                \n",
    "                # read and parse xml files\n",
    "                tree = ET.parse(filepath)\n",
    "                root = tree.getroot()\n",
    "                \n",
    "                full_data.append(root)\n",
    "                \n",
    "                # label text with entities \n",
    "                entities = []\n",
    "                for elem in root[1]:\n",
    "                    # filter non usable entries\n",
    "                    if (elem.get('start') != None) and (elem.get('end') != None) and (elem.get('start') != '-1') and (elem.get('end') != '-1'):\n",
    "                        new_ent = (int(elem.get('start')), int(elem.get('end')), elem.tag)\n",
    "                        entities.append(new_ent)\n",
    "                    if elem.tag not in poss_tags:\n",
    "                        poss_tags.append(elem.tag)\n",
    "                        \n",
    "                # save the 2 special xml files extra so we can access them easier later\n",
    "                if filepath.endswith(\"Bicycles.xml\"):\n",
    "                    print(\"confirmed\")\n",
    "                    special1 = root\n",
    "                elif filepath.endswith(\"Highlights_of_the_Prado_Museum.xml\"):\n",
    "                    print(\"confirmed\")\n",
    "                    special2 = root\n",
    "\n",
    "                        \n",
    "                train_data.append((root[0].text, {'entities': entities}))\n",
    "    \n",
    "# print(TRAIN_DATA[1][0])\n",
    "print(poss_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stable-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model 'en_core_web_sm'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "23-Aug-2002 -- On 23-August-2002 at 8 am we left...\" with entities \"[(1006, 1011, 'PLACE'), (1058, 1063, 'PLACE'), (12...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Cartagena and the Colombian Presidential Election...\" with entities \"[(158, 166, 'PLACE'), (2026, 2032, 'PLACE'), (2080...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Guatemala to Placencia Belize – 6 days, 274 miles...\" with entities \"[(1100, 1106, 'PLACE'), (115, 122, 'PLACE'), (1277...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Durango to San Luis Potosi – 6 days, 313 miles\n",
      "Th...\" with entities \"[(1055, 1062, 'PLACE'), (1106, 1112, 'PLACE'), (11...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Guerrero Negro to Loreto â€“ 4 days, 268 miles\n",
      "Mo...\" with entities \"[(1307, 1312, 'PLACE'), (1325, 1331, 'PLACE'), (14...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "19-Apr-2012 -- Because the weather was fine and ...\" with entities \"[(116, 123, 'PLACE'), (361, 369, 'PLACE'), (377, 3...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "        Museo del Prado\n",
      "        Madrid’s pride, t...\" with entities \"[(1052, 1054, 'PLACE'), (1101, 1125, 'PLACE'), (11...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "        Highlights of the Prado Museum\n",
      "        Sp...\" with entities \"[(10024, 10031, 'PLACE'), (10036, 10043, 'PLACE'),...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Lima to Cusco – dirt roads in the Andes\n",
      "Thursday,...\" with entities \"[(1146, 1152, 'PLACE'), (1737, 1741, 'PLACE'), (21...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "15-Aug-2003 -- On 15-August-2003 our journey to ...\" with entities \"[(102, 107, 'PLACE'), (1065, 1071, 'PLACE'), (1077...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "26-Dec-2002 -- This confluence can be found 1,5 ...\" with entities \"[(1125, 1131, 'PLACE'), (1133, 1142, 'PLACE'), (11...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "The 1999 Floods in Venezuela\n",
      "Tuesday, July 18th, ...\" with entities \"[(107, 114, 'PLACE'), (131, 137, 'PLACE'), (277, 2...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "07-May-2002 -- English On our way home from Orad...\" with entities \"[(1016, 1020, 'PLACE'), (1036, 1043, 'PLACE'), (11...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Melting Glaciers and a Climb of Vallunaraju\n",
      "Monda...\" with entities \"[(1166, 1172, 'PLACE'), (1195, 1206, 'PLACE'), (12...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Medellin to Bogota\n",
      "Monday, June 12th, 2006\n",
      "   I e...\" with entities \"[(1152, 1160, 'PLACE'), (1260, 1269, 'PLACE'), (14...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Mexico City and Bicycles\n",
      "Wednesday, February 15th...\" with entities \"[(139, 150, 'PLACE'), (1806, 1810, 'PLACE'), (193,...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "The Melting Spine of the Andes\n",
      "Wednesday, January...\" with entities \"[(1028, 1039, 'PLACE'), (1210, 1215, 'PLACE'), (12...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "\n",
      "19-Apr-2012 -- The conditions of our further tri...\" with entities \"[(104, 111, 'PLACE'), (141, 146, 'PLACE'), (306, 3...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "        Bourbon Madrid\n",
      "        West of Puerta del...\" with entities \"[(1012, 1045, 'PLACE'), (1067, 1111, 'PLACE'), (11...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "/home/xa/.local/lib/python3.8/site-packages/spacy/training/iob_utils.py:139: UserWarning: [W030] Some entities could not be aligned in the text \"\n",
      "Peru\n",
      "Monday, October 16th, 2006\n",
      "\n",
      "   As I said in ...\" with entities \"[(1013, 1017, 'PLACE'), (108, 115, 'PLACE'), (1195...\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 999\tLosses: 89.442"
     ]
    }
   ],
   "source": [
    "# train the model with our training data\n",
    "\n",
    "model = \"en_core_web_sm\"\n",
    "n_iter = 1000\n",
    "\n",
    "if model is not None:\n",
    "    nlp = spacy.load(model) \n",
    "    print(\"Loaded model '%s'\" % model)\n",
    "else:\n",
    "    nlp = spacy.blank('en')  \n",
    "    print(\"Created blank 'en' model\")\n",
    "\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('ner', last=True)\n",
    "if 'tok2vec' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('tok2vec', last=True)\n",
    "if 'tagger' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('tagger', last=True)\n",
    "if 'parser' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('parser', last=True)\n",
    "if 'sentencizer' not in nlp.pipe_names:\n",
    "    nlp.add_pipe('sentencizer', last=True)\n",
    "\n",
    "\n",
    "ner = nlp.get_pipe('ner')\n",
    "\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get('entities'):\n",
    "        ner.add_label(ent[2])\n",
    "     \n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    # optimizer = nlp.begin_training()\n",
    "    for itn in range(n_iter):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        doc = nlp.make_doc(root[0].text)\n",
    "        for text, annotations in train_data:\n",
    "            try:\n",
    "                nlp.update(\n",
    "                    [Example.from_dict(nlp.make_doc(text), annotations)],  \n",
    "                    drop=0.4,  \n",
    "                    # sgd=optimizer,\n",
    "                    losses=losses)\n",
    "            except:\n",
    "                pass\n",
    "        print('\\rEpisode {}\\tLosses: {:.2f}'.format(itn, losses['ner']), end=\"\")\n",
    "\n",
    "# nlp.to_disk(root+\"/my_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agricultural-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively just load pretrained English tokenizer, tagger, parser and NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "logical-alliance",
   "metadata": {},
   "source": [
    "## 2.3 Auswertung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-circle",
   "metadata": {},
   "source": [
    "Wie oft kommen welche PoS-Tags vor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "useful-apparatus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aufgabe 1:\n",
      "('PUNCT', 3508)\n",
      "('NOUN', 5180)\n",
      "('ADP', 3105)\n",
      "('VERB', 2927)\n",
      "('NUM', 629)\n",
      "('PRON', 1627)\n",
      "('SCONJ', 299)\n",
      "('CCONJ', 826)\n",
      "('AUX', 897)\n",
      "('X', 25)\n",
      "('INTJ', 12)\n",
      "('PART', 518)\n",
      "('SPACE', 825)\n",
      "('SYM', 23)\n",
      "('DET', 2951)\n",
      "('ADJ', 1828)\n",
      "('PROPN', 1927)\n",
      "('ADV', 1315)\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        pos.append(token.pos_)\n",
    "pos = count_pos(pos)\n",
    "print(\"Aufgabe 1:\")\n",
    "print(*pos.items(), sep=\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-diploma",
   "metadata": {},
   "source": [
    "Wie viele [SpatialEntities, Places, Motions, Locations, Signals, QsLinks, OLinks] gibt es?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "considerable-cuisine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aufgabe 2:\n",
      "('MEASURE', 170)\n",
      "('URL', 17)\n",
      "('SPATIAL_ENTITY', 1417)\n",
      "('CP', 17)\n",
      "('MOTION', 771)\n",
      "('METALINK', 1788)\n",
      "('PLACE', 1852)\n",
      "('QSLINK', 970)\n",
      "('OLINK', 244)\n",
      "('MLINK', 42)\n",
      "('MEASURELINK', 93)\n",
      "('PATH', 434)\n",
      "('MOVELINK', 803)\n",
      "('NONMOTION_EVENT', 341)\n",
      "('MOTION_SIGNAL', 526)\n",
      "('SPATIAL_SIGNAL', 714)\n"
     ]
    }
   ],
   "source": [
    "# From xml data\n",
    "ents = []\n",
    "for data in full_data:\n",
    "    for elem in data[1]:\n",
    "         ents.append(elem.tag)\n",
    "\n",
    "ents = count_pos(ents)\n",
    "print(\"Aufgabe 2:\")\n",
    "print(*ents.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defined-smith",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aufgabe 2:\n",
      "('MEASURE', 162)\n",
      "('SPATIAL_ENTITY', 1326)\n",
      "('MOTION', 770)\n",
      "('PLACE', 1600)\n",
      "('PATH', 406)\n",
      "('NONMOTION_EVENT', 340)\n",
      "('MOTION_SIGNAL', 520)\n",
      "('SPATIAL_SIGNAL', 687)\n"
     ]
    }
   ],
   "source": [
    "# From processing the texts with spacy, after training with own data\n",
    "# Not all data could be fed e.g. QSLinks and Olinks\n",
    "ents2 = []\n",
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    for ent in doc.ents:\n",
    "         ents2.append(ent.label_)\n",
    "\n",
    "ents2 = count_pos(ents2)\n",
    "print(\"Aufgabe 2:\")\n",
    "print(*ents2.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-printing",
   "metadata": {},
   "source": [
    "Wie oft kommen welche QsLink Typen vor? (DC,EC, ...)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "continent-rings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aufgabe 3:\n",
      "('', 2)\n",
      "('IN', 586)\n",
      "('NTPP', 42)\n",
      "('OUT', 3)\n",
      "('EC', 196)\n",
      "('TPP', 53)\n",
      "('DC', 41)\n",
      "('PO', 12)\n",
      "('EQ', 35)\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for data in full_data:\n",
    "    for elem in data[1]:\n",
    "        if elem.tag == \"QSLINK\":\n",
    "            links.append(elem.get('relType'))\n",
    "            \n",
    "links = count_pos(links)\n",
    "print(\"Aufgabe 3:\")\n",
    "print(*links.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-policy",
   "metadata": {},
   "source": [
    "Verteilung der Satzlänge graphisch darstellen (x: Satzlänge, y: Wie häufig)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "governmental-cooper",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sents = []\n",
    "for text, _ in train_data:\n",
    "    doc = nlp(text)\n",
    "    for sent in doc.sents:\n",
    "        # find length by counting whitespaces in the sentence\n",
    "        temp1 = sent.text\n",
    "        temp2 = temp1.replace(' ', '')\n",
    "        spaces = len(temp1) - len(temp2)\n",
    "        sents.append(spaces+1)\n",
    "# map lengths to amount of times it appereard\n",
    "distribution = count_pos(sents)\n",
    "plt.bar(list(distribution.keys()), list(distribution.values()))\n",
    "plt.title(\"Satzlängen\")\n",
    "plt.xlabel(\"Satzlängen\")\n",
    "plt.ylabel(\"Häufigkeiten\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legendary-simulation",
   "metadata": {},
   "source": [
    "Welche Links (QSLinks, OLinks) werden von welchen Präpositionen (markiert durch SPATIAL_SIGNAL) getriggert (z.B. wie oft werden QSLinks durch die Präposition „on“ getriggert)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "skilled-amber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aufgabe 5:\n",
      "\n",
      "Qslink trigger:\n",
      "('On', 4)\n",
      "('In', 22)\n",
      "('to', 2)\n",
      "('bordering', 1)\n",
      "('inside of', 1)\n",
      "('covering', 1)\n",
      "('overlooking', 2)\n",
      "('on top of', 1)\n",
      "('for', 1)\n",
      "('about', 1)\n",
      "('into', 1)\n",
      "('inhabited', 1)\n",
      "('beside', 1)\n",
      "('up', 1)\n",
      "('on', 75)\n",
      "('Along', 1)\n",
      "('over', 2)\n",
      "('far from', 3)\n",
      "('afar', 1)\n",
      "('at', 54)\n",
      "('after', 1)\n",
      "('away from', 4)\n",
      "('behind', 1)\n",
      "('adjacent to', 1)\n",
      "('apart', 1)\n",
      "('through', 4)\n",
      "('stocked', 1)\n",
      "('part         of', 1)\n",
      "('Down', 1)\n",
      "('where', 68)\n",
      "('between', 5)\n",
      "('covered', 5)\n",
      "('atop', 2)\n",
      "('packed with', 1)\n",
      "('across', 2)\n",
      "('restricted', 1)\n",
      "('line', 1)\n",
      "('out of', 1)\n",
      "('inside', 4)\n",
      "('apart from', 1)\n",
      "('connects', 4)\n",
      "('upon', 1)\n",
      "('house', 1)\n",
      "('surmounted', 1)\n",
      "('further', 1)\n",
      "('with', 15)\n",
      "('coiling up', 1)\n",
      "('In front of', 1)\n",
      "('up to', 1)\n",
      "('next to', 10)\n",
      "('has', 2)\n",
      "('of', 44)\n",
      "('around', 4)\n",
      "('Everywhere', 1)\n",
      "('from', 3)\n",
      "('filled', 3)\n",
      "('including', 3)\n",
      "('directly beneath', 1)\n",
      "('At', 6)\n",
      "('surrounded', 2)\n",
      "('contains', 1)\n",
      "('along', 14)\n",
      "('full of', 10)\n",
      "('houses', 11)\n",
      "('away', 2)\n",
      "('in front of', 1)\n",
      "('outside', 2)\n",
      "('contain', 4)\n",
      "('on top', 2)\n",
      "('under', 2)\n",
      "('surrounding', 3)\n",
      "('against', 1)\n",
      "('packed', 1)\n",
      "('in', 228)\n",
      "\n",
      "Olink trigger:\n",
      "('On', 2)\n",
      "('to', 3)\n",
      "('across from', 1)\n",
      "('above', 4)\n",
      "('east', 2)\n",
      "('overlooking', 3)\n",
      "('overlook', 1)\n",
      "('south', 4)\n",
      "('in the direction to', 1)\n",
      "('in that direction', 1)\n",
      "('backwards', 1)\n",
      "('south of', 2)\n",
      "('beside', 1)\n",
      "('up', 5)\n",
      "('upstream from', 1)\n",
      "('facing', 1)\n",
      "('on', 45)\n",
      "('northeast', 1)\n",
      "('Along', 1)\n",
      "('over', 7)\n",
      "('east of', 3)\n",
      "('South\\xadeast of', 1)\n",
      "('beneath', 4)\n",
      "('neighboring', 1)\n",
      "('behind', 3)\n",
      "('adjacent to', 1)\n",
      "('southwest', 1)\n",
      "('toward', 1)\n",
      "('Down', 2)\n",
      "('between', 8)\n",
      "('covered', 5)\n",
      "('atop', 2)\n",
      "('on your left', 1)\n",
      "('across', 1)\n",
      "('line', 1)\n",
      "('upon', 1)\n",
      "('alongside', 1)\n",
      "('surmounted', 1)\n",
      "('to SW', 1)\n",
      "('coiling up', 1)\n",
      "('Facing', 1)\n",
      "('In front of', 1)\n",
      "('up to', 1)\n",
      "('next to', 12)\n",
      "('on the right', 1)\n",
      "('West of', 1)\n",
      "('to the west from', 1)\n",
      "('of', 3)\n",
      "('around', 3)\n",
      "('down', 5)\n",
      "('Southeast of', 2)\n",
      "('north of', 2)\n",
      "('to the left', 1)\n",
      "('surrounding', 3)\n",
      "('directly beneath', 1)\n",
      "('next door to', 1)\n",
      "('surrounded', 4)\n",
      "('Across', 1)\n",
      "('along', 14)\n",
      "('in front of', 2)\n",
      "('west', 1)\n",
      "('on top', 2)\n",
      "('below', 4)\n",
      "('under', 3)\n",
      "('north', 2)\n",
      "('on top of', 1)\n",
      "('in', 2)\n"
     ]
    }
   ],
   "source": [
    "qs_trigger = []\n",
    "o_trigger = []\n",
    "\n",
    "# collect all triggers\n",
    "for data in full_data:\n",
    "    temp_qs = []\n",
    "    temp_o = []\n",
    "    trigg_dic = {}\n",
    "    \n",
    "    # add all triggers ONLY with their id\n",
    "    for elem in data[1]:\n",
    "        if elem.tag == \"QSLINK\":\n",
    "            temp_qs.append(elem.attrib['trigger'])\n",
    "        elif elem.tag == \"OLINK\":\n",
    "            temp_o.append(elem.attrib['trigger'])\n",
    "        if elem.tag == \"SPATIAL_SIGNAL\":\n",
    "            trigg_dic[elem.get('id')] = elem.get('text')\n",
    "\n",
    "    # replace trigger id with their respective texts\n",
    "    for i in range(len(temp_qs)):\n",
    "        try:\n",
    "            qs_trigger.append(trigg_dic[temp_qs[i]])\n",
    "        except:\n",
    "            pass\n",
    "    for i in range(len(temp_o)):\n",
    "        try:\n",
    "            o_trigger.append(trigg_dic[temp_o[i]])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "qs_trigger = count_pos(qs_trigger)\n",
    "o_trigger = count_pos(o_trigger)\n",
    "\n",
    "print(\"Aufgabe 5:\")\n",
    "print(\"\\nQslink trigger:\", *qs_trigger.items(), sep=\"\\n\")\n",
    "print(\"\\nOlink trigger:\", *o_trigger.items(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "global-hybrid",
   "metadata": {},
   "source": [
    "Welches sind die fünf häufigsten „MOTION“ Verben (und wie oft kommen diese vor)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cardiac-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma_verbs = []   # only collect lemma from verbs\n",
    "for data in full_data:\n",
    "    # collect all different motion verbs\n",
    "    verbs = []  \n",
    "    for elem in data[1]:\n",
    "        if elem.tag == \"MOTION\":\n",
    "            verbs.append(elem.attrib['text'])\n",
    "            \n",
    "    # only add their lemma to the actual list\n",
    "    doc = nlp(data[0].text)\n",
    "    for token in doc:\n",
    "        if token.text in verbs:\n",
    "            lemma_verbs.append(token.lemma_)\n",
    "\n",
    "lemma_verbs = count_pos(lemma_verbs)\n",
    "lemma_verbs = sorted(lemma_verbs.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Aufgabe 6:\", *lemma_verbs[0:5], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "respective-sharing",
   "metadata": {},
   "source": [
    "## 2.4 Visualisierung"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convenient-patient",
   "metadata": {},
   "source": [
    "Graphische Darstellung von Verbindungen zwischen Entitäten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equipped-essay",
   "metadata": {},
   "source": [
    "**Bicycle.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "derived-gateway",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(special1)\n",
    "\n",
    "G = nx.Graph()\n",
    "color_map = []\n",
    "counter = 0\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"SPATIAL_ENTITY\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='r', with_labels=True)\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"PLACE\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='b', with_labels=True)\n",
    "\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"LOCATION\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='g', with_labels=True)\n",
    "\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"PATH\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='w', with_labels=True)\n",
    "\n",
    "\n",
    "for elem in special1[1]:\n",
    "     if elem.tag == \"NONMOTIONEVENT\":\n",
    "          G.add_node(elem.attrib['text'])\n",
    "\n",
    "nx.draw(G, node_color='y', with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-great",
   "metadata": {},
   "source": [
    "**Highlights_of_the_Prado_Museum.xml**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-advisory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-melbourne",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comparative-chain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hazardous-finnish",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
